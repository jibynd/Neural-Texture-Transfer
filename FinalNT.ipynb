{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalNT.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1zGG89oiH5eR","colab_type":"text"},"cell_type":"markdown","source":["# Neural Texture Transfer"]},{"metadata":{"id":"vQKTnlOxH2Pq","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (16,20)\n","mpl.rcParams['axes.grid'] = False\n","import numpy as np\n","from PIL import Image\n","import scipy.misc\n","\n","import tensorflow as tf\n","import tensorflow.contrib.eager as tfe\n","from tensorflow.python.keras.preprocessing import image as kp_image\n","from tensorflow.python.keras import models\n","#from TextUtils import *\n","from PIL import Image\n","from tensorflow.python.keras.preprocessing import image as k_process\n","import IPython.display"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YJ7E-CaGIBmc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"19878e30-47ba-4f7a-b965-d2f22d5804a8","executionInfo":{"status":"ok","timestamp":1542880431485,"user_tz":480,"elapsed":6600,"user":{"displayName":"Jiby Nd","photoUrl":"","userId":"15692248352648630983"}}},"cell_type":"code","source":["tf.enable_eager_execution()\n","print(\"Eager execution enabled: {}\".format(tf.executing_eagerly()))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Eager execution enabled: True\n"],"name":"stdout"}]},{"metadata":{"id":"sqGft9cKIGZP","colab_type":"code","colab":{}},"cell_type":"code","source":["content_path = 'cathd.jpg'; texture_path = 'carbon crack.jpg'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8Esal5hdIJxl","colab_type":"code","colab":{}},"cell_type":"code","source":["def slicer(path_to_img, mode = None, size = None, split = (3,3)):\n","    img = Image.open(path_to_img)\n","    if mode is \"texture\":\n","        img = img.resize(size).convert('L').convert('RGB')\n","    imgwidth, imgheight = img.size\n","    n_split_w, n_split_h = split\n","    height = imgheight//n_split_h; width = imgwidth//n_split_w\n","    #print('slice', height, width)\n","    imgs = []\n","    for i in range(0,n_split_h*height,height):\n","        for j in range(0,n_split_w*width,width):\n","            box = (j, i, j+width, i+height)\n","            imgs.append(np.array(img.crop(box)))\n","    return np.array(imgs).astype('uint8')\n","  \n"," \n","def recomp(img):\n","  #tf, np = imp()\n","  up = np.concatenate(img[:3], axis = 1)\n","  mid = np.concatenate(img[3:6], axis = 1)\n","  down = np.concatenate(img[6:], axis = 1)\n","  #print(up.shape, mid.shape, down.shape)\n","  tout = np.concatenate([up ,mid, down], axis = 0)\n","  tout = Image.fromarray(tout)\n","  return tout"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JKXpbqW1UyYF","colab_type":"code","colab":{}},"cell_type":"code","source":["def turn_bw(img):\n","    img = tf.image.rgb_to_grayscale(img)\n","    img_bw = tf.image.grayscale_to_rgb(img)\n","    return img_bw"],"execution_count":0,"outputs":[]},{"metadata":{"id":"epk04qZ3SS50","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(texture_layers, content_layers):\n","  #from tensorflow.python.keras import models\n","  #tf, np = imp()\n","  # Load our model. We load pretrained VGG, trained on imagenet data\n","  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","  # Get output layers corresponding to style and content layers\n","  texture_outputs = [vgg.get_layer(name).output for name in texture_layers]\n","  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n","  model_outputs = texture_outputs + content_outputs\n","  # Build model\n","  return models.Model(vgg.input, model_outputs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SmhySbhT0FbD","colab_type":"code","colab":{}},"cell_type":"code","source":["def deprocess_img(processed_img):\n","  #tf, np = imp()\n","  x = processed_img.copy()\n","  if len(x.shape) == 0:\n","    x = np.squeeze(x, 0)\n","  assert len(x.shape) == 4, (\"Input to deprocess image must be an image of \"\n","                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n","  if len(x.shape) != 4:\n","    raise ValueError(\"Invalid input to deprocessing image\")\n","\n","  # perform the inverse of the preprocessiing step\n","  x[:, :, :, 0] += 103.939\n","  x[:, :, :, 1] += 116.779\n","  x[:, :, :, 2] += 123.68\n","  x = x[:, :, :, ::-1]\n","\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lMvQvMfKSYRL","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_img(img):\n","  #from PIL import Image\n","  #from tensorflow.python.keras.preprocessing import image as k_process\n","  #tf, np = imp()\n","  max_dim = 512\n","  #img = Image.open(path_to_img)\n","  long = max(img.shape)\n","  b, h, w, c = img.shape\n","  scale = max_dim/long\n","  hr , wr = round(h*scale), round(w*scale)\n","  #print('hr', hr, wr)\n","  ar = k_process.img_to_array; pics = Image.fromarray\n","  img = np.array([ar(pics(d).resize((wr, hr), Image.ANTIALIAS)) for d in img])\n","\n","  return img\n","\n","\n","def load_and_process_img(img):\n","  #tf, np = imp()\n","  img = load_img(img)\n","  img = tf.keras.applications.vgg19.preprocess_input(img)\n","  return img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y4XNAnrhTL7P","colab_type":"code","colab":{}},"cell_type":"code","source":["def gram_matrix(input_tensor):\n","    batch,  height, width, channels = [int(d) for d in input_tensor.shape]\n","    tensor = tf.reshape(input_tensor, [batch, height*width, channels])\n","    # Get the product height*width\n","    n = tf.shape(tensor)[1]\n","    gram = tf.matmul(tensor, tensor, transpose_a=True)\n","    #gram = tf.nn.dropout(gram, 0.5, seed = 1)\n","    #gram = tf.nn.max_pool(tf.reshape(gram,[1,channels,channels,1]),ksize = [1,2, 2,1], strides=[1, 1, 1, 1],padding= 'VALID')\n","    return gram / tf.cast(n, tf.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UyvfJAikS8TX","colab_type":"code","colab":{}},"cell_type":"code","source":["  def get_loss(tensor1, tensor2):\n","    reduce_space = list(range(1,len(tensor1.shape)))\n","    #print(ls)\n","    return tf.reduce_mean(tf.square(tensor1 - tensor2), reduce_space)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t5MrJF9CVtkd","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_deep_outputs(model, inputs, num_texture_layers):\n","    \n","    t = int(inputs.shape[0])//2\n","    outputs = model(inputs)\n","    color_outputs = [layer[:t] for layer in outputs[num_texture_layers:]]\n","    bw_outputs = [gram_matrix(layer[t:2*t]) for layer in outputs[:num_texture_layers]]\n","    #img_outputs = [layer[2*t:3*t] for layer in outputs[num_texture_layers:]]\n","    #img_bw_outputs = [gram_matrix(layer[3*t:]) for layer in outputs[:num_texture_layers]]\n","    \n","    return color_outputs, bw_outputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rd4y-QxbStbL","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_loss(model, loss_weights, img_sync, const_outputs, content_image):\n","    \n","    content_output, texture_output  = const_outputs\n","    num_texture_layers = len(texture_output)\n","    num_content_layers = len(content_output)\n","    \n","    img_sync_bw = turn_bw(img_sync) # copy!!!\n","    var_inputs = tf.concat([img_sync, img_sync_bw], axis = 0)\n","    img_sync_output, img_sync_bw_output = get_deep_outputs(model, var_inputs, num_texture_layers)\n","    \n","    texture_loss = 0\n","    content_loss = 0\n","    \n","    # Accumulate texture losses from all layers\n","    w_text = 1.0 / float(num_texture_layers)\n","    for texture_feat, var_bw_feat in zip(texture_output, img_sync_bw_output):\n","        texture_loss += w_text * get_loss(texture_feat, var_bw_feat)\n","    \n","    # Accumulate content losses from all layers\n","    w_cont = 1.0 / float(num_content_layers)\n","    for content_feat, var_feat in zip(content_output, img_sync_output):\n","        content_loss += w_cont * get_loss(content_feat, var_feat)\n","    \n","    direct_weight, texture_weight, content_weight = loss_weights\n","    texture_loss *= texture_weight\n","    content_loss *= content_weight\n","    \n","    direct_loss = direct_weight*get_loss(img_sync, content_image)\n","   \n","    total_loss = texture_loss + content_loss + direct_loss\n","    \n","    return total_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S9bWgULBkdsr","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_grads(args):\n","  with tf.GradientTape() as tape: \n","    loss = compute_loss(**args)\n","  return tape.gradient(loss, args['img_sync']), loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ud1lryKifLZG","colab_type":"code","colab":{}},"cell_type":"code","source":["content_path = 'cathd.jpg'; texture_path = 'carbon crack.jpg'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4_wgz6k6fHWm","colab_type":"code","colab":{}},"cell_type":"code","source":["content_layers = ['block1_conv1', 'block5_conv2'] \n","content_layers = ['block5_conv2']\n","# Style layer we are interested in\n","texture_layers = ['block1_conv1',\n","                  'block2_conv1'#,'block3_conv1'#, 'block4_conv1'#, 'block5_conv1'\n","               ]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0OpwyOVVTqOj","colab_type":"code","colab":{}},"cell_type":"code","source":["def neural_texture_train(content_path, texture_path, split = (2,2),loss_weights = (1e3,1e3, 1e-3), num_iterations=1000,\n","                        layers = ()):\n","    \n","    texture_layers, content_layers = layers\n","    num_content_layers = len(content_layers)\n","    num_texture_layers = len(texture_layers)\n","    \n","    model = get_model(texture_layers, content_layers) \n","    for layer in model.layers:\n","        layer.trainable = False\n","        \n","    content_image = slicer(content_path, split=split)\n","    content_image = load_and_process_img(content_image)\n","    \n","    size = Image.open(content_path).size\n","    texture_image = slicer(texture_path, size = size, mode='texture', split=split)\n","    texture_image = load_and_process_img(texture_image)\n","    \n","    img_sync = tfe.Variable(content_image, dtype=tf.float32)\n","    \n","    \n","    const_inputs = tf.concat([content_image, texture_image], axis = 0)\n","    const_outputs = get_deep_outputs(model, const_inputs, num_texture_layers)\n","    \n","    opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n","    \n","    args = {'model': model,'loss_weights': loss_weights,'img_sync': img_sync, \n","            'const_outputs': const_outputs, 'content_image': content_image, }\n","    \n","    norm_means = np.array([103.939, 116.779, 123.68])# norms(init_image.shape)\n","    min_vals = -norm_means\n","    max_vals = 255 - norm_means\n","    display_interval = 10\n","    for i in range(num_iterations):\n","        print('Iteration ', i)\n","        grads, loss = get_grads(args)\n","        opt.apply_gradients([(grads, img_sync)])\n","        clipped = tf.clip_by_value(img_sync, min_vals, max_vals)\n","        img_sync.assign(clipped)\n","        \n","        if i % display_interval == 0:\n","            # Use the .numpy() method to get the concrete numpy array\n","            plot_img = img_sync.numpy()\n","            plot_img = deprocess_img(plot_img)\n","            #scipy.misc.imsave('outfile.jpg', recomp(plot_img))\n","            IPython.display.clear_output(wait=False)\n","            IPython.display.display_png(Image.fromarray(plot_img[1]))\n","            print('Iteration: {}'.format(i))        \n","            print('Total loss: {:.4e}, ' )\n","    \n","    pics = deprocess_img(img_sync.numpy())\n","    return recomp(pics)\n","    \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"CEWxozY1m5yy","colab_type":"code","colab":{}},"cell_type":"code","source":["neural_texture_train(content_path, texture_path, loss_weights = (5e3,1e3, 1e-3), num_iterations=1000,\n","                        split = (3,3), layers = (content_layers, texture_layers))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1YC7kZQC0kNq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}